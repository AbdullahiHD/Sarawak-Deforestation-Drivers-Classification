{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Imports and Configuration\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.cluster import KMeans\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"model_path\": \"Trained_Models/EfficientNet.pthpth\",\n",
    "    \"csv_file\": \"SarawakDataset/2017-2018.csv\",\n",
    "    \"root_dir\": \"SarawakDataset\",\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 4,\n",
    "    \"num_classes\": 4,\n",
    "    \"output_file\": \"SarawakDataset/predictions.csv\",\n",
    "}\n",
    "\n",
    "print(\"Configuration and imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "class UnlabeledForestDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        lat = self.data.iloc[idx, 0]\n",
    "        long = self.data.iloc[idx, 1]\n",
    "        event_year = int(self.data.iloc[idx, 2])\n",
    "        example_path = self.data.iloc[idx, 3]\n",
    "\n",
    "        folder = example_path.split(\"/\")[-1]\n",
    "        filename_prefix = folder.replace(\".\", \"_\")\n",
    "\n",
    "        images = []\n",
    "        for i in range(4):\n",
    "            current_year = event_year + 1 + i\n",
    "            image_filename = f\"{filename_prefix}_{current_year}.png\"\n",
    "            image_path = os.path.join(self.root_dir, folder, \"images\", image_filename)\n",
    "\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "            image = io.imread(image_path)\n",
    "\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented[\"image\"]\n",
    "\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image = torch.from_numpy(image).type(torch.float)\n",
    "                image = image.permute(2, 0, 1)\n",
    "\n",
    "            image = image[:, 86:246, 86:246]\n",
    "            images.append(image)\n",
    "\n",
    "        images = torch.stack(images)  # Shape: [4, 3, 160, 160]\n",
    "\n",
    "        slope_path = os.path.join(self.root_dir, folder, \"auxiliary\", \"srtm.npy\")\n",
    "        if not os.path.exists(slope_path):\n",
    "            raise FileNotFoundError(f\"Slope file not found: {slope_path}\")\n",
    "\n",
    "        slope = np.load(slope_path)\n",
    "        slope = torch.from_numpy(slope).type(torch.float)\n",
    "        slope = slope[86:246, 86:246]\n",
    "\n",
    "        return images, slope, (lat, long, event_year, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_data_loader(config):\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(270),\n",
    "            A.ElasticTransform(p=0.4, alpha=120, sigma=120 * 0.05, alpha_affine=None),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = UnlabeledForestDataset(\n",
    "        csv_file=config[\"csv_file\"], root_dir=config[\"root_dir\"], transform=transform\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Set to 0 to run in the main process\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Batch image shape: torch.Size([16, 4, 3, 160, 160])\n",
      "Batch slope shape: torch.Size([16, 160, 160])\n",
      "\n",
      "Sample 2:\n",
      "Batch image shape: torch.Size([16, 4, 3, 160, 160])\n",
      "Batch slope shape: torch.Size([16, 160, 160])\n",
      "\n",
      "Sample 3:\n",
      "Batch image shape: torch.Size([16, 4, 3, 160, 160])\n",
      "Batch slope shape: torch.Size([16, 160, 160])\n",
      "\n",
      "DataLoader returns:\n",
      "images shape: torch.Size([16, 4, 3, 160, 160])\n",
      "slopes shape: torch.Size([16, 160, 160])\n",
      "metadata length: 4\n"
     ]
    }
   ],
   "source": [
    "# Data shape check\n",
    "data_loader = get_data_loader(CONFIG)\n",
    "dataiter = iter(data_loader)\n",
    "\n",
    "for i in range(3):  # Check first 3 samples\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    images, slopes, metadata = next(dataiter)\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch slope shape: {slopes.shape}\")\n",
    "    # print(f\"Metadata: {metadata}\")\n",
    "\n",
    "print(\"\\nDataLoader returns:\")\n",
    "print(f\"images shape: {images.shape}\")\n",
    "print(f\"slopes shape: {slopes.shape}\")\n",
    "print(f\"metadata length: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading function defined.\n"
     ]
    }
   ],
   "source": [
    "# Model Loading\n",
    "from model.effnet import *\n",
    "\n",
    "def load_model(config):\n",
    "    model = EffnetMLP(\"efficientnet-b2\", config[\"num_classes\"])\n",
    "    model.load_state_dict(\n",
    "        state_dict=torch.load(\n",
    "            config[\"model_path\"], map_location=config[\"device\"], weights_only=False)\n",
    "    )\n",
    "    model = model.to(config[\"device\"])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Model loading function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def predict_and_save_results(model, data_loader, config):\n",
    "    results = []\n",
    "    all_features = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, slopes, metadata) in enumerate(data_loader):\n",
    "            try:\n",
    "                images = images.to(config[\"device\"])  # Shape: [B, 4, 3, 160, 160]\n",
    "                slopes = slopes.to(config[\"device\"])  # Shape: [B, 160, 160]\n",
    "\n",
    "                batch_size, num_years, channels, height, width = images.shape\n",
    "\n",
    "                # Process each year separately\n",
    "                batch_features = []\n",
    "                for year in range(num_years):\n",
    "                    year_image = images[:, year, :, :, :]\n",
    "\n",
    "                    # Get the features (before the final classification layer)\n",
    "                    x = model.extract(year_image).squeeze(-1).squeeze(-1)\n",
    "                    x = torch.flatten(x, 1)\n",
    "                    x = model.linear_img(x)\n",
    "\n",
    "                    s = model.flatten(slopes)\n",
    "                    s = model.batch1d_n(s)\n",
    "                    s = model.dropout(s)\n",
    "                    s = model.batch1d(F.leaky_relu(model.linear1(s)))\n",
    "                    s = F.leaky_relu(model.linear2(s))\n",
    "\n",
    "                    features = torch.cat((x, s), dim=1)\n",
    "                    features = model.linear_fin(features)\n",
    "\n",
    "                    batch_features.append(features)\n",
    "\n",
    "                # Average the features across years\n",
    "                batch_features = torch.stack(batch_features).mean(dim=0)\n",
    "                all_features.append(batch_features.cpu().numpy())\n",
    "\n",
    "                # Process metadata\n",
    "                for i in range(len(metadata[0])):\n",
    "                    lat, long, year, folder = (\n",
    "                        metadata[0][i],\n",
    "                        metadata[1][i],\n",
    "                        metadata[2][i],\n",
    "                        metadata[3][i],\n",
    "                    )\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"latitude\": float(lat),\n",
    "                            \"longitude\": float(long),\n",
    "                            \"year\": int(year),\n",
    "                            \"folder\": str(folder),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Processed {batch_idx * config['batch_size']} samples\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Concatenate all features\n",
    "    features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "    # Use K-means clustering to assign labels\n",
    "    kmeans = KMeans(n_clusters=config[\"num_classes\"], random_state=42)\n",
    "    predicted_labels = kmeans.fit_predict(features)\n",
    "\n",
    "    # Map cluster numbers to class names\n",
    "    label_map = {\n",
    "        0: \"Grassland shrubland\",\n",
    "        1: \"Other\",\n",
    "        2: \"Plantation\",\n",
    "        3: \"Smallholder agriculture\",\n",
    "    }\n",
    "\n",
    "    # Add predictions to results\n",
    "    for i, result in enumerate(results):\n",
    "        result[\"predicted_label\"] = label_map[predicted_labels[i]]\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(config[\"output_file\"], index=False, float_format=\"%.6f\")\n",
    "    print(f\"Results saved to {config['output_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure verified.\n",
      "Data loader created successfully\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "Model loaded successfully\n",
      "Processed 0 samples\n",
      "Results saved to SarawakDataset/predictions2_output.csv\n",
      "Prediction process completed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Check dataset structure\n",
    "    sample_row = pd.read_csv(CONFIG[\"csv_file\"]).iloc[0]\n",
    "    folder_name = sample_row[\"example_path\"].split(\"/\")[-1]\n",
    "    filename_prefix = folder_name.replace(\".\", \"_\")\n",
    "    event_year = int(sample_row[\"year\"])\n",
    "    sample_path = os.path.join(\n",
    "        CONFIG[\"root_dir\"],\n",
    "        folder_name,\n",
    "        \"Images\",\n",
    "        f\"{filename_prefix}_{event_year + 1}.png\",\n",
    "    )\n",
    "    if not os.path.exists(sample_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Sample image not found: {sample_path}. Please check your dataset structure and CONFIG settings.\"\n",
    "        )\n",
    "    print(\"Dataset structure verified.\")\n",
    "\n",
    "    # Load data\n",
    "    data_loader = get_data_loader(CONFIG)\n",
    "    print(\"Data loader created successfully\")\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(CONFIG)\n",
    "    print(\"Model loaded successfully\")\n",
    "\n",
    "    # Print loaded state dict keys\n",
    "    state_dict = torch.load(\n",
    "        CONFIG[\"model_path\"], map_location=CONFIG[\"device\"], weights_only=False\n",
    "    )\n",
    "\n",
    "    # Make predictions and save results\n",
    "    predict_and_save_results(model, data_loader, CONFIG)\n",
    "\n",
    "    print(\"Prediction process completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during execution: {str(e)}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
